\section{Linear Regression} \label{Linear Regression}

In this section, multiple linear regression is applied to the Auto MPG, House Prices, and Insurance Charges data sets. We evaluated the model using in-sample, out-of-sample, and cross validation. Finally, we applied forward selection, backward elimination, and stepwise selection to identify which features are optimal for explanation of the response variable.

\subsection{Auto MPG}

\Cref{tab:Scalation - AutoMPG Linear Regression} presents the quality of fit metrics for the in-sample and out-of-sample evaluations using scalation, while \Cref{tab:Statsmodels - Auto MPG Linear Regression} presents the same metrics using statsmodels. We can see that regression is performing decently, and that the main statistics are similar for both scalation and mathstats.

\begin{table}[H]
\centering
\caption{Scalation - AutoMPG Linear Regression}
\label{tab:Scalation - AutoMPG Linear Regression}
\begin{tabular}{|c|c|c|} \hline 
Metric & In-Sample & 80-20 Split \\ \hline \hline 
rSq &0.809255 & 0.822842 \\ \hline 
rSqBar &0.806283 &      0.820081 \\ \hline 
sst &23819.0 &  4731.23 \\ \hline 
sse &4543.35 &  838.174 \\ \hline 
sde &3.40878 &  3.29026 \\ \hline 
mse0 &11.5902 & 10.7458 \\ \hline 
rmse &3.40443 & 3.27808 \\ \hline 
mae &2.61826 &  2.48735 \\ \hline 
smape &12.0589 &  11.8858 \\ \hline 
m &392.000 &    78.0000 \\ \hline 
dfr &6.00000 &  6.00000 \\ \hline 
df &385.000 &   385.000 \\ \hline 
fStat &272.234 &        298.034 \\ \hline 
aic &-1022.45 & -189.284 \\ \hline 
bic &-994.656 & -172.787 \\ \hline 
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Statsmodels - Auto MPG Linear Regression}
\label{tab:Statsmodels - Auto MPG Linear Regression}
\begin{tabular}{|c|c|c|}\hline
Metric & In-Sample & 80-20 Split \\ \hline \hline
rSq & 0.8093 & 0.7942 \\ \hline
rSqBar & 0.8063 & 0.7801 \\ \hline
sst & 23818.9935 & 4032.2061 \\ \hline
sse & 4543.3470 & 829.6873 \\ \hline
sde & 3.4352 & 3.3713 \\ \hline
mse0 & 11.8009 & 10.5024 \\ \hline
rmse & 3.4352 & 3.2407 \\ \hline
mae & 2.6183 & 2.5039 \\ \hline
smape & 12.0589 & 12.3880 \\ \hline
m & 392.0000 & 79.0000 \\ \hline
dfr & 6.0000 & 6.0000 \\ \hline
df & 385.0000 & 73.0000 \\ \hline
fStat & 272.2341 & 46.9622 \\ \hline
aic & 2086.9095 & 197.7765 \\ \hline
bic & 2114.7083 & 211.9932 \\ \hline
\end{tabular}
\end{table}

\Cref{fig:Scalation - AutoMPG reg} shows that plots for the predicted $y$-values vs. the actual $y$-values from scalation, while \Cref{fig:Statsmodels - AutoMPG reg} show the same from mathstats. Again the results are very similar.

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Auto_MPG/scalation_reg_In_Sample.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Auto_MPG/scalation_reg_80_20.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Scalation - Auto MPG Regression\\ Left: In Sample Predictions\\ Right: 80-20 Out of Sample Predictions\\ yy black/actual vs. yp red/predicted}
    \label{fig:Scalation - AutoMPG reg}
\end{figure}

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{Auto_MPG/statsmodels_reg_In_Sample.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{Auto_MPG/statsmodels_reg_80_20.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Statsmodels - Auto MPG Regression\\ Left: In Sample Predictions\\ Right: 80-20 Out of Sample Predictions\\ yy black/actual vs. yp red/predicted}
    \label{fig:Statsmodels - AutoMPG reg}
\end{figure}

Next, we performed 5 fold cross validation. \Cref{tab: Scalation - Auto MPG Linear Regression CV,tab:Statsmodels - Auto MPG Linear Regression CV} show the resulting quality of fit metrics from scalation and statsmodels respectively. Again we see that the results are similar.

\begin{table}[H]
\centering
\caption{Scalation - Auto MPG Linear Regression CV}
\label{tab: Scalation - Auto MPG Linear Regression CV}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline
Name & num & min & max & mean & stdev & interval \\ \hline \hline
rSq & 5 & 0.788 & 0.823 & 0.798 & 0.014 & 0.018 \\ \hline
rSqBar & 5 & 0.785 & 0.820 & 0.795 & 0.014 & 0.018 \\ \hline
sst & 5 & 3962.818 & 5671.580 & 4700.481 & 620.767 & 770.935 \\ \hline
sse & 5 & 824.554 & 1176.435 & 950.494 & 142.696 & 177.215 \\ \hline
sde & 5 & 3.177 & 3.738 & 3.431 & 0.226 & 0.281 \\ \hline
mse0 & 5 & 10.571 & 15.083 & 12.186 & 1.829 & 2.272 \\ \hline
rmse & 5 & 3.251 & 3.884 & 3.483 & 0.256 & 0.318 \\ \hline
mae & 5 & 2.487 & 2.850 & 2.689 & 0.151 & 0.188 \\ \hline
smape & 5 & 11.886 & 12.905 & 12.372 & 0.427 & 0.530 \\ \hline
m & 5 & 78.000 & 78.000 & 78.000 & 0.000 & 0.000 \\ \hline
dfr & 5 & 6.000 & 6.000 & 6.000 & 0.000 & 0.000 \\ \hline
df & 5 & 385.000 & 385.000 & 385.000 & 0.000 & 0.000 \\ \hline
fStat & 5 & 239.054 & 298.034 & 254.430 & 24.517 & 30.448 \\ \hline
aic & 5 & -205.110 & -188.647 & -194.539 & 6.676 & 8.291 \\ \hline
bic & 5 & -188.613 & -172.150 & -178.042 & 6.676 & 8.291 \\ \hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Statsmodels - Auto MPG Linear Regression CV}
\label{tab:Statsmodels - Auto MPG Linear Regression CV}
\begin{tabular}{|c|c|c|c|c|c|}\hline
Name & In-num folds & min & max & mean & stdev \\ \hline \hline
rSq & 5 & 0.7654 & 0.8282 & 0.8010 & 0.0216 \\ \hline
rSqBar & 5 & 0.7491 & 0.8163 & 0.7873 & 0.0231 \\ \hline
sst & 5 & 4032.2061 & 5792.1365 & 4724.2751 & 617.8288 \\ \hline
sse & 5 & 745.1709 & 1359.0577 & 947.8346 & 213.7052 \\ \hline
sde & 5 & 3.2171 & 4.3446 & 3.5980 & 0.3912 \\ \hline
mse0 & 5 & 9.5535 & 17.4238 & 12.0958 & 2.7627 \\ \hline
rmse & 5 & 3.0909 & 4.1742 & 3.4576 & 0.3756 \\ \hline
mae & 5 & 2.5039 & 3.1904 & 2.6786 & 0.2601 \\ \hline
smape & 5 & 11.2379 & 14.1325 & 12.3805 & 0.9795 \\ \hline
m & 5 & 78.0000 & 79.0000 & 78.4000 & 0.4899 \\ \hline
dfr & 5 & 6.0000 & 6.0000 & 6.0000 & 0.0000 \\ \hline
df & 5 & 72.0000 & 73.0000 & 72.4000 & 0.4899 \\ \hline
fStat & 5 & 39.1425 & 57.8612 & 49.2658 & 6.3699 \\ \hline
aic & 5 & 188.0386 & 234.9114 & 205.6271 & 15.7223 \\ \hline
bic & 5 & 202.1788 & 249.0516 & 219.7980 & 15.7128 \\ \hline
\end{tabular}
\end{table}

Finally, we apply forward selection, backward elimination, and stepwise selection to determine which variables best explain response variable. \Cref{fig:Scalation - AutoMPG reg FS} shows plots for $R^2$, $\overline{R^2}$, sMAPE, $R^2$ cv, and AIC vs. $n$ (the number of variables selected) when utilizing forward selection. From the left plot, it is clear that only $2$ variables are needed to explain the response variable (even just $1$ variable is not too bad either). Those variables $2$ variables are weight and modelyear. The order in which the variables were chosen was 1. weight, 2. modelyear, 3. cylinders, 4. acceleration, 5. horsepower, and 6. displacment.

One interesting thing to note is how the graph of AIC vs. $n$ is always increasing.

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Auto_MPG/scalation_reg_FS_R2.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Auto_MPG/scalation_reg_FS_aic.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Scalation - Auto MPG Regression Forward Selection\\ Left: $R^2$ vs. $n$\\ Right: aic vs. $n$}
    \label{fig:Scalation - AutoMPG reg FS}
\end{figure}

\Cref{fig:Scalation - AutoMPG reg BE} shows plots for $R^2$, $\overline{R^2}$, sMAPE, $R^2$ cv, and AIC vs. $n$ (the number of variables selected) when utilizing backward elimination. From the left plot, it is again clear that only $2$ variables are needed to explain the response variable (and again even just $1$ variable is not too bad either). Here, both backward and forward selection agree with each other in their selection of variables, so if you were to remove one variable at a time, in order you would remove 1. displacment, 2. horsepower, 3. acceleration, 4. cylinders, 5. modelyear, and 6. weight. This is the reverse order of forward selection, meaning that they agree.

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Auto_MPG/scalation_reg_BE_R2.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Auto_MPG/scalation_reg_BE_aic.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Scalation - Auto MPG Regression Backward Elimination\\ Left: $R^2$ vs. $n$\\ Right: aic vs. $n$}
    \label{fig:Scalation - AutoMPG reg BE}
\end{figure}

Last, \Cref{fig:Scalation - AutoMPG reg SS} shows plots for $R^2$, $\overline{R^2}$, sMAPE, $R^2$ cv, and AIC vs. $n$ (the number of variables selected) when utilizing stepwise selection. Here, since stepwise can move forward and backwards which results in multiple different metric evaluations for models that use the same number of variables, (I believe) the evaluation that is plotted for $n$ variables is the best evaluation that stepwise selection found for all the models with $n$ variables that it tested. Despite this, we still find that if we want to get the best models recommended from stepwise, we should add in the following order, 1. weight, 2. modelyear, 3. cylinders, 4. acceleration, and 5. horsepower. Note that this almost agrees with forward selection and backward elimination, but we do not add displacement as stepwise decided that moving was not worthwhile.

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Auto_MPG/scalation_reg_SS_R2.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Auto_MPG/scalation_reg_SS_aic.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Scalation - Auto MPG Regression Stepwise Selection\\ Left: $R^2$ vs. $n$\\ Right: aic vs. $n$}
    \label{fig:Scalation - AutoMPG reg SS}
\end{figure}

\subsection{House Prices}

\Cref{tab:Scalation - House Price Linear Regression} presents the quality of fit metrics for the in-sample and out-of-sample evaluations using scalation, while \Cref{tab:Statsmodels - House Price Linear Regression} presents the same metrics using statsmodels. We can see that regression is performing extremely well, almost perfectly capturing the data. The main statistics are similar for both scalation and mathstats.

\begin{table}[H]
\centering
\caption{Scalation - House Price Linear Regression}
\label{tab:Scalation - House Price Linear Regression}
\begin{tabular}{|c|c|c|} \hline 
Metric & In-Sample & 80-20 Split \\ \hline \hline 
rSq &0.998516 & 0.998649 \\ \hline 
rSqBar &0.998507 &      0.998641 \\ \hline 
sst &6.42325e+13 &      1.33700e+13 \\ \hline 
sse &9.53030e+10 &      1.80638e+10 \\ \hline 
sde &9767.21 &  9501.56 \\ \hline 
mse0 &9.53030e+07 &     9.03190e+07 \\ \hline 
rmse &9762.32 & 9503.63 \\ \hline 
mae &7747.66 &  7484.48 \\ \hline 
smape &1.57791 &        1.60847 \\ \hline 
m &1000.00 &    200.000 \\ \hline 
dfr &6.00000 &  6.00000 \\ \hline 
df &993.000 &   993.000 \\ \hline 
fStat &111378 & 122330 \\ \hline 
aic &-10591.2 & -2101.67 \\ \hline 
bic &-10556.9 & -2078.59 \\ \hline 
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Statsmodels - House Price Linear Regression}
\label{tab:Statsmodels - House Price Linear Regression}
\begin{tabular}{|c|c|c|}\hline
Metric & In-Sample & 80-20 Split \\ \hline \hline
rSq & 0.9985 & 0.9984 \\ \hline
rSqBar & 0.9985 & 0.9984 \\ \hline
sst & 64232463468052.5469 & 12891771417242.6445 \\ \hline
sse & 95249090298.3967 & 20286959701.1265 \\ \hline
sde & 9798.8381 & 10252.5012 \\ \hline
mse0 & 96017228.1234 & 101434798.5056 \\ \hline
rmse & 9798.8381 & 10071.4844 \\ \hline
mae & 7740.4301 & 8174.5836 \\ \hline
smape & 1.5774 & 1.6620 \\ \hline
m & 1000.0000 & 200.0000 \\ \hline
dfr & 7.0000 & 7.0000 \\ \hline
df & 992.0000 & 193.0000 \\ \hline
fStat & 95425.1583 & 17493.2676 \\ \hline
aic & 21225.8831 & 3700.9854 \\ \hline
bic & 21265.1451 & 3724.0736 \\ \hline
\end{tabular}
\end{table}

\Cref{fig:Scalation - Housing reg} shows that plots for the predicted $y$-values vs. the actual $y$-values from scalation, while \Cref{fig:Statsmodels - Housing reg} show the same from mathstats. Again the results are very similar.

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Housing/scalation_reg_In_Sample.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Housing/scalation_reg_80_20.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Scalation - House Price Regression\\ Left: In Sample Predictions\\ Right: 80-20 Out of Sample Predictions\\ yy black/actual vs. yp red/predicted}
    \label{fig:Scalation - Housing reg}
\end{figure}

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{Housing/statsmodels_reg_In_Sample.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{Housing/statsmodels_reg_80_20.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Statsmodels - House Price Regression\\ Left: In Sample Predictions\\ Right: 80-20 Out of Sample Predictions\\ yy black/actual vs. yp red/predicted}
    \label{fig:Statsmodels - Housing reg}
\end{figure}

Next, we performed 5 fold cross validation. \Cref{tab: Scalation - House Price Linear Regression CV,tab:Statsmodels - House Price Linear Regression CV} show the resulting quality of fit metrics from scalation and statsmodels respectively. Again we see that the results are similar.

\begin{table}[H]
\centering
\caption{Scalation - House Price Linear Regression CV}
\label{tab: Scalation - House Price Linear Regression CV}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline 
Name &  num folds & min & max & mean & stdev & interval \\ \hline \hline 
rSq & 5 & 0.998 & 0.999 & 0.998 & 0.000 & 0.000 \\ \hline 
rSqBar & 5 & 0.998 & 0.999 & 0.998 & 0.000 & 0.000 \\ \hline 
sst & 5 & 11805100457351.200 &13369989427686.710 & 12829460198984.865 & 606159535374.424 & 752793908917.133 \\ \hline 
sse & 5 & 17790794374.705 & 22663619750.593 & 19394433560.564 & 2117948592.845 & 2630295668.134 \\ \hline 
sde & 5 &   9365.942 &  10608.530 &   9818.348 &    533.068 &    662.021 \\ \hline 
mse0 & 5 & 88953971.874 & 113318098.753 & 96972167.803 & 10589742.964 & 13151478.341 \\ \hline 
rmse & 5 &   9431.541 &  10645.097 &   9836.093 &    528.486 &    656.330 \\ \hline 
mae & 5 &   7418.536 &   8609.308 &   7817.189 &    534.033 &    663.219 \\ \hline 
smape & 5 & 1.408 &      1.804 &      1.592 &      0.141 &      0.176 \\ \hline 
m & 5 & 200.000 &    200.000 &    200.000 &      0.000 &      0.000 \\ \hline 
dfr & 5 & 6.000 &      6.000 &      6.000 &      0.000 &      0.000 \\ \hline 
df & 5 & 993.000 &    993.000 &    993.000 &      0.000 &      0.000 \\ \hline 
fStat & 5 & 96001.466 & 122329.999 & 110142.703 &  10843.190 &  13466.236 \\ \hline 
aic & 5 & -2127.278 &  -2100.155 &  -2109.082 &     11.789 &     14.641 \\ \hline 
bic & 5 & -2104.190 &  -2077.067 &  -2085.993 &     11.789 &     14.641 \\ \hline 
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Statsmodels - House Price Linear Regression CV}
\label{tab:Statsmodels - House Price Linear Regression CV}
\begin{tabular}{|c|c|c|c|c|c|}\hline
Name & In-num folds & min & max & mean & stdev \\ \hline \hline
rSq & 5 & 0.9984 & 0.9986 & 0.9985 & 0.0001 \\ \hline
rSqBar & 5 & 0.9984 & 0.9986 & 0.9985 & 0.0001 \\ \hline
sst & 5 & 12384264865084.2500 & 13564576490393.4668 & 12842056251568.3359 & 395911885307.9127 \\ \hline
sse & 5 & 17238729843.0851 & 20996608313.5931 & 19277864710.9665 & 1296174615.0581 \\ \hline
sde & 5 & 9450.9176 & 10430.2788 & 9988.5569 & 337.6978 \\ \hline
mse0 & 5 & 86193649.2154 & 104983041.5680 & 96389323.5548 & 6480873.0753 \\ \hline
rmse & 5 & 9284.0535 & 10246.1232 & 9812.2003 & 331.7354 \\ \hline
mae & 5 & 7516.9137 & 8174.5836 & 7794.6342 & 224.6310 \\ \hline
smape & 5 & 1.5104 & 1.6620 & 1.5879 & 0.0540 \\ \hline
m & 5 & 200.0000 & 200.0000 & 200.0000 & 0.0000 \\ \hline
dfr & 5 & 7.0000 & 7.0000 & 7.0000 & 0.0000 \\ \hline
df & 5 & 193.0000 & 193.0000 & 193.0000 & 0.0000 \\ \hline
fStat & 5 & 17493.2676 & 20276.9346 & 18399.0661 & 1001.2402 \\ \hline
aic & 5 & 3668.4214 & 3707.8619 & 3690.3225 & 13.5977 \\ \hline
bic & 5 & 3691.5096 & 3730.9501 & 3713.4107 & 13.5977 \\ \hline
\end{tabular}
\end{table}

Finally, we apply forward selection, backward elimination, and stepwise selection to determine which variables best explain response variable. \Cref{fig:Scalation - Housing reg FS} shows plots for $R^2$, $\overline{R^2}$, sMAPE, $R^2$ cv, and AIC vs. $n$ (the number of variables selected) when utilizing forward selection. From the left plot, it is clear that only $1$ variable is needed to explain the response variable. The order in which the variables were chosen was 1. Square Footage, 2. Year Built, 3. Lot Size, 4. Num Bedrooms, 5. Num Bathrooms, and 6. Garage Size.

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Housing/scalation_reg_FS_R2.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Housing/scalation_reg_FS_aic.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Scalation - House Prices Regression Forward Selection\\ Left: $R^2$ vs. $n$\\ Right: aic vs. $n$}
    \label{fig:Scalation - Housing reg FS}
\end{figure}

\Cref{fig:Scalation - Housing reg BE} shows plots for $R^2$, $\overline{R^2}$, sMAPE, $R^2$ cv, and AIC vs. $n$ (the number of variables selected) when utilizing backward elimination. From the left plot, it is again clear that only $1$ variable is needed to explain the response variable. Here, both backward and forward selection agree with each other in their selection of variables, so if you were to remove one variable at a time, in order you would remove 1. Garage Size, 2. Num Bathrooms, 3. Num Bedrooms, 4. Lot Size, 5. Year Built, and 6. Square Footage. This is the reverse order of forward selection, meaning that they agree.

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Housing/scalation_reg_BE_R2.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Housing/scalation_reg_BE_aic.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Scalation - House Prices Regression Backward Elimination\\ Left: $R^2$ vs. $n$\\ Right: aic vs. $n$}
    \label{fig:Scalation - Housing reg BE}
\end{figure}

Last, \Cref{fig:Scalation - Housing reg SS} shows plots for $R^2$, $\overline{R^2}$, sMAPE, $R^2$ cv, and AIC vs. $n$ (the number of variables selected) when utilizing stepwise selection. We still find that if we want to get the best models recommended from stepwise, we should add in the following order, 1. Square Footage, 2. Year Built, 3. Lot Size, 4. Num Bedrooms, and 5. Num Bathrooms. Note that this almost agrees with forward selection and backward elimination, but we do not add Garage Size as stepwise decided that moving was not worthwhile.

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Housing/scalation_reg_SS_R2.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Housing/scalation_reg_SS_aic.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Scalation - House Prices Regression Stepwise Selection\\ Left: $R^2$ vs. $n$\\ Right: aic vs. $n$}
    \label{fig:Scalation - Housing reg SS}
\end{figure}

\subsection{Insurance Charges}

\Cref{tab:Scalation - Insurance Charges Linear Regression} presents the quality of fit metrics for the in-sample and out-of-sample evaluations using scalation, while \Cref{tab:Statsmodels - Insurance Charges Linear Regression} presents the same metrics using statsmodels. We can see that regression is performing okay, and that the main statistics are similar for both scalation and mathstats.

\begin{table}[H]
\centering
\caption{Scalation - Insurance Charges Linear Regression}
\label{tab:Scalation - Insurance Charges Linear Regression}
\begin{tabular}{|c|c|c|} \hline 
Metric & In-Sample & 80-20 Split \\ \hline \hline 
rSq &0.750157 & 0.720005 \\ \hline 
rSqBar &0.748842 &      0.718531 \\ \hline 
sst &1.96074e+11 &      4.06432e+10 \\ \hline 
sse &4.89878e+10 &      1.13799e+10 \\ \hline 
sde &6053.11 &  6540.46 \\ \hline 
mse0 &3.66127e+07 &     4.26213e+07 \\ \hline 
rmse &6050.84 & 6528.50 \\ \hline 
mae &4179.54 &  4430.66 \\ \hline 
smape &37.9722 &        40.0602 \\ \hline 
m &1338.00 &    267.000 \\ \hline 
dfr &7.00000 &  7.00000 \\ \hline 
df &1330.00 &   1330.00 \\ \hline 
fStat &570.477 &        488.584 \\ \hline 
aic &-13533.8 & -2708.17 \\ \hline 
bic &-13492.2 & -2679.47 \\ \hline 
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\caption{Statsmodels - Insurance Charges Linear Regression}
\label{tab:Statsmodels - Insurance Charges Linear Regression}
\begin{tabular}{|c|c|c|}\hline
Metric & In-Sample & 80-20 Split \\ \hline \hline
rSq & 0.7509 & 0.7836 \\ \hline
rSqBar & 0.7494 & 0.7778 \\ \hline
sst & 196074221568.3671 & 41606660039.7953 \\ \hline
sse & 48839532843.9219 & 9003973448.1649 \\ \hline
sde & 6062.1023 & 5884.7827 \\ \hline
mse0 & 36749084.1564 & 33596915.8514 \\ \hline
rmse & 6062.1023 & 5796.2847 \\ \hline
mae & 4170.8869 & 4181.1945 \\ \hline
smape & 37.8059 & 40.0220 \\ \hline
m & 1338.0000 & 268.0000 \\ \hline
dfr & 8.0000 & 8.0000 \\ \hline
df & 1329.0000 & 260.0000 \\ \hline
fStat & 500.8107 & 117.6800 \\ \hline
aic & 27113.5058 & 4660.4252 \\ \hline
bic & 27160.2962 & 4689.1531 \\ \hline
\end{tabular}
\end{table}

\Cref{fig:Scalation - Insurance reg} shows that plots for the predicted $y$-values vs. the actual $y$-values from scalation, while \Cref{fig:Statsmodels - Insurance reg} show the same from mathstats. Again the results are very similar.


\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Insurance/scalation_reg_In_Sample.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Insurance/scalation_reg_80_20.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Scalation - Insurance Charges Regression\\ Left: In Sample Predictions\\ Right: 80-20 Out of Sample Predictions\\ yy black/actual vs. yp red/predicted}
    \label{fig:Scalation - Insurance reg}
\end{figure}

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{Insurance/statsmodels_reg_In_Sample.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{Insurance/statsmodels_reg_80_20.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Statsmodels - Insurance Charges Regression\\ Left: In Sample Predictions\\ Right: 80-20 Out of Sample Predictions\\ yy black/actual vs. yp red/predicted}
    \label{fig:Statsmodels - Insurance reg}
\end{figure}

Next, we performed 5 fold cross validation. \Cref{tab: Scalation - Insurance Charges Linear Regression CV,tab:Statsmodels - Insurance Charges Linear Regression CV} show the resulting quality of fit metrics from scalation and statsmodels respectively. Again we see that the results are similar.

\begin{table}[H]
\centering
\caption{Scalation - Insurance Charges Linear Regression CV}
\label{tab: Scalation - Insurance Charges Linear Regression CV}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline 
Name &  num & min & max & mean & stdev & interval \\ \hline \hline 
rSq & 5 & 0.701 &      0.814 &      0.743 &      0.046 &      0.057 \\ \hline 
rSqBar & 5 & 0.699 &      0.813 &      0.742 &      0.046 &      0.057 \\ \hline 
sst & 5 & 31902777173.848 & 43430486230.657 & 38949749086.422 & 4343029725.651 & 5393640012.108 \\ \hline 
sse & 5 & 7539480548.420 & 11454966761.392 & 9918152392.401 & 1670500799.560 & 2074607019.047 \\ \hline 
sde & 5 & 5320.957 &   6562.018 &   6084.654 &    526.821 &    654.263 \\ \hline 
mse0 & 5 & 28237754.863 & 42902497.234 & 37146638.174 & 6256557.302 & 7770063.742 \\ \hline 
rmse & 5 & 5313.921 &   6550.000 &   6076.688 &    525.006 &    652.009 \\ \hline 
mae & 5 & 3810.754 &   4511.498 &   4216.703 &    292.688 &    363.491 \\ \hline 
smape & 5 & 36.128 &     40.060 &     38.143 &      1.833 &      2.277 \\ \hline 
m & 5 & 267.000 &    267.000 &    267.000 &      0.000 &      0.000 \\ \hline 
dfr & 5 & 7.000 &      7.000 &      7.000 &      0.000 &      0.000 \\ \hline 
df & 5 & 1330.000 &   1330.000 &   1330.000 &     0.000 &      0.000 \\ \hline 
fStat & 5 & 444.882 &    830.519 &    573.015 &    157.534 &    195.643 \\ \hline 
aic & 5 & -2709.047 &  -2663.110 &  -2691.017 &     19.599 &     24.340 \\ \hline 
bic & 5 & -2680.349 &  -2634.412 &  -2662.319 &     19.599 &     24.340 \\ \hline 
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Statsmodels - Insurance Charges Linear Regression CV}
\label{tab:Statsmodels - Insurance Charges Linear Regression CV}
\begin{tabular}{|c|c|c|c|c|c|}\hline
Name & In-num folds & min & max & mean & stdev \\ \hline \hline
rSq & 5 & 0.6324 & 0.7956 & 0.7402 & 0.0578 \\ \hline
rSqBar & 5 & 0.6225 & 0.7901 & 0.7332 & 0.0593 \\ \hline
sst & 5 & 30189024179.7055 & 43857198758.4016 & 39154186092.5516 & 4823002859.8713 \\ \hline
sse & 5 & 8965018845.2774 & 11096336332.7613 & 9899546225.2180 & 821555419.0107 \\ \hline
sde & 5 & 5872.0390 & 6545.4562 & 6170.1628 & 260.9758 \\ \hline
mse0 & 5 & 33451562.8555 & 41559312.1077 & 36998683.9155 & 3128575.4360 \\ \hline
rmse & 5 & 5783.7326 & 6446.6512 & 6077.2269 & 256.8986 \\ \hline
mae & 5 & 4054.1099 & 4427.9335 & 4203.4121 & 129.0554 \\ \hline
smape & 5 & 35.6194 & 40.0220 & 38.1279 & 1.5723 \\ \hline
m & 5 & 267.0000 & 268.0000 & 267.6000 & 0.4899 \\ \hline
dfr & 5 & 8.0000 & 8.0000 & 8.0000 & 0.0000 \\ \hline
df & 5 & 259.0000 & 260.0000 & 259.6000 & 0.4899 \\ \hline
fStat & 5 & 55.7054 & 126.4912 & 97.8508 & 24.6138 \\ \hline
aic & 5 & 4659.2632 & 4699.8828 & 4678.3124 & 16.0624 \\ \hline
bic & 5 & 4687.9911 & 4728.5808 & 4707.0283 & 16.0528 \\ \hline
\end{tabular}
\end{table}

Finally, we apply forward selection, backward elimination, and stepwise selection to determine which variables best explain response variable. \Cref{fig:Scalation - Insurance reg FS} shows plots for $R^2$, $\overline{R^2}$, sMAPE, $R^2$ cv, and AIC vs. $n$ (the number of variables selected) when utilizing forward selection. From the left plot, it is clear that only $s$ variables are needed to explain the response variable. The order in which the variables were chosen was 1. smoker\_yes, 2. age, 3. bmi, 4. children, 5. region\_southeast, 6. sex\_male, and 7. region\_northwest

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Insurance/scalation_reg_FS_R2.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Insurance/scalation_reg_FS_aic.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Scalation - Insurance Charges Regression Forward Selection\\ Left: $R^2$ vs. $n$\\ Right: aic vs. $n$}
    \label{fig:Scalation - Insurance reg FS}
\end{figure}

\Cref{fig:Scalation - Insurance reg BE} shows plots for $R^2$, $\overline{R^2}$, sMAPE, $R^2$ cv, and AIC vs. $n$ (the number of variables selected) when utilizing backward elimination. From the left plot, it is again clear that only $2$ variables are needed to explain the response variable. Here, both backward and forward selection agree with each other in their selection of variables, so if you were to remove one variable at a time, in order you would remove 1. region\_northwest, 2. sex\_male, 3. region\_southeast, 4. children, 5. bmi, 6. age, and 7. smoker\_yes. This is the reverse order of forward selection, meaning that they agree.

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Insurance/scalation_reg_BE_R2.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Insurance/scalation_reg_BE_aic.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Scalation - Insurance Charges Regression Backward Elimination\\ Left: $R^2$ vs. $n$\\ Right: aic vs. $n$}
    \label{fig:Scalation - Insurance reg BE}
\end{figure}

Last, \Cref{fig:Scalation - Insurance reg SS} shows plots for $R^2$, $\overline{R^2}$, sMAPE, $R^2$ cv, and AIC vs. $n$ (the number of variables selected) when utilizing stepwise selection. We still find that if we want to get the best models recommended from stepwise, we should add in the following order, 1. smoker\_yes, 2. age, 3. bmi, 4. children, 5. region\_southeast, and 6. sex\_male. Note that this almost agrees with forward selection and backward elimination, but we do not add region\_northwest as stepwise decided that moving was not worthwhile.

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Insurance/scalation_reg_SS_R2.png} 
        %\caption{Description of left image}
        %\label{fig:left}
    \end{subfigure}
    \hfill % Adds flexible space between the images
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Insurance/scalation_reg_SS_aic.png}
        %\caption{Description of right image}
        %\label{fig:right}
    \end{subfigure}
    \caption{Scalation - Insurance Charges Regression Stepwise Selection\\ Left: $R^2$ vs. $n$\\ Right: aic vs. $n$}
    \label{fig:Scalation - Insurance reg SS}
\end{figure}


























